# GeoSpider分布式爬虫

第六届中软杯赛题——分布式爬虫系统

------

**GeoSpider** 是一款适用于结构化抓取新闻博客、电商类网站的分布式爬虫。系统可对新闻网站，绝大多数的博客和电商网站进行数据自动摘录。采用多进程辅佐分布式的并发方式，提高采集效率。总体使用Psutil进行进程、Scrapy-Redis作分布式与管理、MonoDB作数据存储。爬虫任务可设定启动时间、结束时间、选择主机IP、进程数量等。爬虫代码分为两部分，bigcrawler和geowind_crawler，bigcrawler为爬虫应用，部署在每台分布式从机上，geowind_crawler为爬虫管理系统的Web应用，部署在一台服务器上即可，两者基于Redis的发布订阅进行通信，实现Web应用对爬虫应用的管理。
#### 项目部署地址[http://123.207.230.48/crawlermanage/]
用户名：admin   密码：a

#### 视频链接 [ http://www.bilibili.com/video/av12055194/ ]
如弹幕影响观看，可关闭弹幕观看

## 主要功能
本节主要介绍 **GeoSpider** 的基本功能，包括数据的抓取、状态监控和简单的数据分析等等。
### 1. 数据抓取
 **GeoSpider** 是一个为爬虫系统，旨在高效、准确地抓取网络页面中的数据。同时针对新闻和博客类网站进行一定的优化，尽量减少了人工操作，自动结构化提取。

### 2. 针对反爬
 **GeoSpider** 使用代理IP池、浏览器代理、Webkit内核浏览器模拟等方法，尽可能地针对反爬虫问题。同时，网站登录服务不对外进行注册功能、保障本系统数据的反反爬功能。

### 3. 爬虫状态监控
使用Graphite监控所有爬虫任务，能够监控到爬虫的下载速率、爬去数据数量等，并绘制成图像显示出来。
爬虫启动后，每台主机会启动一个监听进程来监听定时进程，将即将到达结束时间的进程终止。

### 4. 数据分析
1.堆叠条形图：统计正在进行爬虫任务的数量信息，分别统计电商爬虫、新闻爬虫、博客爬虫分别运行、暂停、等待、故障的数量。
2.饼状图：统计历史爬虫（已经结束的爬虫）电商、新闻、博客的数量比例。
3.阶梯折线图：统计最近一周时间电商、新闻、博客爬虫任务的数量走向。
3.雷达图：最有价值的图。统计当前每台机器的爬虫进程数量，用户可以很清晰的获知爬虫任务在每台机器上的分布情况，因此可以分析出每台机器的负荷，为用户下次发布任务提供指导。


## 设计思路
### 1. 电商类网站结构化
#### 1.1、导航栏抓取
返回值：列表（标签:url），例如：（裙子:http://www.xxx.com/qunzi）
##### 设计思路概述
主要思想是是渐进的特征提取，以关键字聚类作为辅助。
【相关方法解释】
特征提取：此处用到的特征提取，主要是是对页面中的关键字、标签特征等等进行提取，例如价格、Search等关键字，class='.*nav.*'等标签语言。
关键字聚类：预先设计好电商网站的导航栏关键字，其次对Web网页中某个结构进行多次匹配，计算得分，得分越高聚类程度越强，选优者作为导航栏，存在一定误差和少量不足。


##### 方法概述
以下方法按照顺序执行，上一个方法不成功则进行下一个。
在全站的url中筛选出是否含有关键字的url
通过“所有商品分类”、“商品分类”、“分类”等关键字的特征值提取，获取大分类页面。若该步成功，跳转到 xx 步继续执行
提取特征：class = *nav*，并进行关键词匹配
提取特征：tag_name = ul，并进行关键词匹配
方法1.2.2与方法1.2.3提取获得的结果将进行再一次的导航栏抓取，由于二级页面可能存在大分类页面或是母标签的扩展标签，尽可能的多提取标签与url。
上述方法都不适用（暂未遇到），拟计划提醒消息，手动输入class或id或其他特征值进行抓取



【注】:获取大分类所有标签（1.1扩展）
主要思想：标签树序列化，聚类后获取量值最优解，选作结果集

标签树序列化后对形态：html/body/div/div/ul/li/a

#### 1.2、导航链接解析（抓取详细页面链接）
（此模块可归为第一步，由于相对复杂，单独列项）
对于第一步（导航栏抓取）拿到的 (名词:url)列表，称为url_list，进行第一遍遍历。

分支一
如果名词（关键字）在url中出现，就可以做关键词查询操作（例如我们平常在购物网站购物时做的那样） （此处需要对该名词的选择还是比较考究的，最好选择有list/search字样的url，选择不慎可能出现大量url没有商品信息的问题。）
1.1 选择较优的url，简称为orignal_url，并对url做如下操作

步骤一：用&分割, split(‘&’)
步骤二：拼接字符串，获得包括关键字在内的近-最简的url，此处需要对url进行拼接试探。获取简化		后的url，以下简称simple_url。
步骤三：对 simple_url 进行解析，包括翻页信息、关键字替换信息等等。
步骤四：对url_list进行第二遍遍历，将里面关键字在simple_url中替换，得到res_url_list


拼接试探：先获取包括url在内的最简化的url，简称为simple_url。利用request等方法测试其html数量上与orignal_url用request获得的数量，若减少量巨大，接上其他由&分割的字串，继续测试。

分支二
如果名词（关键词）始终没有在url中出现（或不符合要求）
直接默认这些url为商品列表，直接规划为res_url_list

#### 1.3、解析商品列表页面
【注】：在遍历之前最好对分支二得到的res_url_list做一遍聚类处理，对每一类进行一遍结构化

进行具体结构化，获取商品的详细页面链接，思路如下：
1、判断该网址中的存储类型，以此选择解析方法：
通过html中的字符串数量判断，共分为以下3中解析方式：
1.1、JSON解析方法（request），主要商品数据在JSON中
1.2、标签解析方法（request），数据不在Json中，在标签中可直接提取
1.3、标签解析方法（webdriver），数据不在Json中，通过request无法获得，借用webdriver获取标签

上述步骤主要获取商品详细页面的url，并进行一定的提取，例如价格、描述等等


#### 1.4、商品详细页面分析

通过对关键词的提取，预先设置关键词的先后顺序，不断匹配html标签中的信息，结构化出内容。例如匹配商品价格时，首先查找


### 2. 新闻博客类网站提取
该正文抽取算法在基于行块分布函数的网页正文抽取方法上做了优化，提高了准确率，使提取的正文更加“一字不差”。在比赛给出的测试包下进行测试，准确率达到90以上。

对于新闻博客类网站，一般文字内容最集中的区域就是正文。但是也不尽然，有些新闻正文很短，而导航栏内容信息很多。首先，过滤噪声标签等的影响。采用正则过滤掉ul、script、style、注释等内容，标记该内容为A，然后过滤所有标签，再标记该内容为B。然后定义k行为一个行块，去掉空格的长度为行块长度。将过滤掉标签的内容B进行行块长度统计，根据行块分布找出最密集的区域则为初步得到的正文内容Text。

该正文内容已经基本正确，但是如果该正文区域下方或前方不远处出现小部分无关内容，也会计入正文内容，导致有稍许误差。为了提高准确率，在去掉噪声的网页A中全文搜索初步正文Text中的块信息，一般为“p“标签，再计算得到该标签的父节点。将所有获得的父节点存储下来，出现次数最多的父节点标签记为包含整个正文区域的标签。直接从该标签提取文字即为正文。该过程失败则使用之前提取的内容Text作为正文，成功则使用该过程提取的文字作为正文。

算法详情：[html-extractor](https://github.com/cnyangkui/html-extractor)

### 3. 分布式与多进程概述
各机器之间的通信借助Redis的订阅发布功能，订阅了爬虫频道的从机才能够收到主机的信息，解析主机的命令，然后控制机器自己上面的爬虫程序。
主机根据用户需求，向指定的从机发送控制爬虫的命令（启动、暂停、唤醒、终止）和该爬虫任务需要启动的进程数量。从机通过控制进程来控制爬虫。

### 4. 错误恢复机制
机器监听到本机上的爬虫由于异常即将终止程序时，该机器会清空本机的所有任务，根据任务记录启动新的爬虫程序继续之前未完成的任务。
已爬取过的链接由于已经存储下来，所有重新启动爬虫后会进行URL判重，不会抓取到重复的链接。


## 主要框架与依赖
> * Scrapy
> * 发布日记，杂文，所见所想
> * 撰写发布技术文稿（代码支持）
> * 撰写发布学术论文（LaTeX 公式支持）
