# GeoSpider分布式爬虫

第六届中软杯赛题——分布式爬虫系统

------

**GeoSpider** 是一款适用于结构化抓取新闻博客、电商类网站的分布式爬虫。系统可对新闻网站，绝大多数的博客和电商网站进行数据自动摘录。采用多进程辅佐分布式的并发方式，提高采集效率。总体使用Psutil进行进程、Scrapy-Redis作分布式与管理、MonoDB作数据存储。爬虫任务可设定启动时间、结束时间、选择主机IP、进程数量等。爬虫代码分为两部分，bigcrawler和geowind_crawler，bigcrawler为爬虫应用，部署在每台分布式从机上，geowind_crawler为爬虫管理系统的Web应用，部署在一台服务器上即可，两者基于Redis的发布订阅进行通信，实现Web应用对爬虫应用的管理。
#### 项目部署地址[http://123.207.230.48/crawlermanage/]
用户名：admin   密码：a

#### 视频链接 [ http://www.bilibili.com/video/av12055194/ ]
如弹幕影响观看，可关闭弹幕观看

## 主要功能
本节主要介绍 **GeoSpider** 的基本功能，包括数据的抓取、状态监控和简单的数据分析等等。
### 1. 数据抓取
 **GeoSpider** 是一个为爬虫系统，旨在高效、准确地抓取网络页面中的数据。同时针对新闻和博客类网站进行一定的优化，尽量减少了人工操作，自动结构化提取。

### 2. 针对反爬
 **GeoSpider** 使用代理IP池、浏览器代理、Webkit内核浏览器模拟等方法，尽可能地针对反爬虫问题。同时，网站登录服务不对外进行注册功能、保障本系统数据的反反爬功能。

### 3. 爬虫状态监控
使用Graphite监控所有爬虫任务，能够监控到爬虫的下载速率、爬去数据数量等，并绘制成图像显示出来。
爬虫启动后，每台主机会启动一个监听进程来监听定时进程，将即将到达结束时间的进程终止。

### 4. 数据分析
1.堆叠条形图：统计正在进行爬虫任务的数量信息，分别统计电商爬虫、新闻爬虫、博客爬虫分别运行、暂停、等待、故障的数量。
2.饼状图：统计历史爬虫（已经结束的爬虫）电商、新闻、博客的数量比例。
3.阶梯折线图：统计最近一周时间电商、新闻、博客爬虫任务的数量走向。
3.雷达图：最有价值的图。统计当前每台机器的爬虫进程数量，用户可以很清晰的获知爬虫任务在每台机器上的分布情况，因此可以分析出每台机器的负荷，为用户下次发布任务提供指导。


## 设计思路
### 1. 电商类网站结构化
#### 1.1、导航栏抓取
返回值：列表（标签:url），例如：（裙子:http://www.xxx.com/qunzi）
##### 设计思路概述
主要思想是是渐进的特征提取，以关键字聚类作为辅助。
【相关方法解释】
特征提取：此处用到的特征提取，主要是是对页面中的关键字、标签特征等等进行提取，例如价格、Search等关键字，class='.*nav.*'等标签语言。
关键字聚类：预先设计好电商网站的导航栏关键字，其次对Web网页中某个结构进行多次匹配，计算得分，得分越高聚类程度越强，选优者作为导航栏，存在一定误差和少量不足。


##### 方法概述
以下方法按照顺序执行，上一个方法不成功则进行下一个。
在全站的url中筛选出是否含有关键字的url
通过“所有商品分类”、“商品分类”、“分类”等关键字的特征值提取，获取大分类页面。若该步成功，跳转到 xx 步继续执行
提取特征：class = *nav*，并进行关键词匹配
提取特征：tag_name = ul，并进行关键词匹配
方法1.2.2与方法1.2.3提取获得的结果将进行再一次的导航栏抓取，由于二级页面可能存在大分类页面或是母标签的扩展标签，尽可能的多提取标签与url。
上述方法都不适用（暂未遇到），拟计划提醒消息，手动输入class或id或其他特征值进行抓取



【注】:获取大分类所有标签（1.1扩展）
主要思想：标签树序列化，聚类后获取量值最优解，选作结果集

标签树序列化后对形态：html/body/div/div/ul/li/a

#### 1.2、导航链接解析（抓取详细页面链接）
（此模块可归为第一步，由于相对复杂，单独列项）
对于第一步（导航栏抓取）拿到的 (名词:url)列表，称为url_list，进行第一遍遍历。

分支一
如果名词（关键字）在url中出现，就可以做关键词查询操作（例如我们平常在购物网站购物时做的那样） （此处需要对该名词的选择还是比较考究的，最好选择有list/search字样的url，选择不慎可能出现大量url没有商品信息的问题。）
1.1 选择较优的url，简称为orignal_url，并对url做如下操作

步骤一：用&分割, split(‘&’)
步骤二：拼接字符串，获得包括关键字在内的近-最简的url，此处需要对url进行拼接试探。获取简化		后的url，以下简称simple_url。
步骤三：对 simple_url 进行解析，包括翻页信息、关键字替换信息等等。
步骤四：对url_list进行第二遍遍历，将里面关键字在simple_url中替换，得到res_url_list


拼接试探：先获取包括url在内的最简化的url，简称为simple_url。利用request等方法测试其html数量上与orignal_url用request获得的数量，若减少量巨大，接上其他由&分割的字串，继续测试。

分支二
如果名词（关键词）始终没有在url中出现（或不符合要求）
直接默认这些url为商品列表，直接规划为res_url_list

#### 1.3、解析商品列表页面
【注】：在遍历之前最好对分支二得到的res_url_list做一遍聚类处理，对每一类进行一遍结构化

进行具体结构化，获取商品的详细页面链接，思路如下：
1、判断该网址中的存储类型，以此选择解析方法：
通过html中的字符串数量判断，共分为以下3中解析方式：
1.1、JSON解析方法（request），主要商品数据在JSON中
1.2、标签解析方法（request），数据不在Json中，在标签中可直接提取
1.3、标签解析方法（webdriver），数据不在Json中，通过request无法获得，借用webdriver获取标签

上述步骤主要获取商品详细页面的url，并进行一定的提取，例如价格、描述等等


#### 1.4、商品详细页面分析

通过对关键词的提取，预先设置关键词的先后顺序，不断匹配html标签中的信息，结构化出内容。例如匹配商品价格时，首先查找


### 2. 新闻博客类网站提取
新闻博客抽取算法是采用基于行块分布函数的网页正文抽取方法，并在该方法上再做了优化，在官方测试包的测试下，准确率达到90以上。

对于新闻博客类网站，一般文字内容最集中的区域就是正文。但是也不尽然，有些新闻正文很短，而导航栏内容信息很多。首先，过滤噪声标签等的影响。采用正则过滤掉ul、script、style、注释等内容，标记该内容为A，然后过滤所有标签，再标记该内容为B。然后定义k行为一个行块，去掉空格的长度为行块长度。将过滤掉标签的内容B进行行块长度统计，根据行块分布找出最密集的区域则为正文内容。

该正文内容已经基本正确，但是如果该正文区域下方或前方不远处出现小部分无关内容，也会计入正文内容，导致有稍许误差。为了提高准确率，在去掉噪声的网页中全文搜索正文A中的块信息，一般为“p“标签，再计算得到该标签的父节点。将所有获得的父节点存储下来，出现次数最多的父节点标签记为包含整个正文区域的标签。直接从该标签提取文字即为正文。该过程失败则使用之前提取的内容A作为正文，成功则使用该区域提取的文字作为正文。

### 3. 分布式与多进程概述
各机器之间的通信借助Redis的订阅发布功能，订阅了爬虫频道的从机才能够收到主机的信息，解析主机的命令，然后控制机器自己上面的爬虫程序。
主机根据用户需求，向指定的从机发送控制爬虫的命令（启动、暂停、唤醒、终止）和该爬虫任务需要启动的进程数量。从机通过控制进程来控制爬虫。

### 4. 错误恢复机制
机器监听到本机上的爬虫由于异常即将终止程序时，该机器会清空本机的所有任务，根据任务记录启动新的爬虫程序继续之前未完成的任务。
已爬取过的链接由于已经存储下来，所有重新启动爬虫后会进行URL判重，不会抓取到重复的链接。


## 主要框架与依赖
> * Scrapy
> * 发布日记，杂文，所见所想
> * 撰写发布技术文稿（代码支持）
> * 撰写发布学术论文（LaTeX 公式支持）
